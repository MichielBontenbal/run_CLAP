{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97a13bc",
   "metadata": {},
   "source": [
    "# Real World UrbanSoundsSamples dataset with CLAP\n",
    "\n",
    "###  Goal of the notebook\n",
    "In this notebook you can do audio classification with CLAP.\n",
    "We will use the real world samples as collected in the fall of 2024. We analyse 50 samples. \n",
    "\n",
    "### CLAP\n",
    "CLAP (Contrastive Language-Audio Pretraining) is a neural network trained on a variety of (audio, text) pairs. It can be instructed in to predict the most relevant text snippet, given an audio, without directly optimizing for the task.\n",
    "\n",
    "Source: https://huggingface.co/laion/larger_clap_general\n",
    "CLAP paper: https://arxiv.org/abs/2211.06687\n",
    "\n",
    "In this notebook we will use two CLAP models:\n",
    "1. larger clap music and speech\n",
    "2. larger clap general\n",
    "\n",
    "In general I believe the larger clap general model gives better results. \n",
    "\n",
    "### Using ðŸ¤— datasets and ðŸ¤—transformers\n",
    "The dataset is hosted on the Huggingface Hub at: https://huggingface.co/datasets/MichielBontenbal/UrbanSoundsII\n",
    "\n",
    "(This is a new version of the same dataset as the old dataset got corrupted.)\n",
    "\n",
    "This dataset contains nine classes of audio events in an urban environment. \n",
    "\n",
    "In this notebook we will use ðŸ¤—  ```dataset``` library to load this dataset. \n",
    "\n",
    "And we'll use the ðŸ¤— ```transformers``` library to run the CLAP model. Please find more info: https://huggingface.co/docs/transformers/model_doc/clap \n",
    "\n",
    "\n",
    "### Contents\n",
    "0. Install packages & check versions\n",
    "1. Inspection of dataset\n",
    "2. Testing one sample of the UrbanSoundsSamples dataset\n",
    "3. Generating results for the whole dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d66e31",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca44ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24640679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc99f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80d4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install datasets\\[audio\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e18c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87dca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check python version\n",
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95407312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f'numpy={np.__version__}')\n",
    "import soundfile\n",
    "print(f'soundfile={soundfile.__version__}')\n",
    "import librosa\n",
    "print(f'librosa={librosa.__version__}')\n",
    "import IPython\n",
    "print(f'ipython={IPython.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "print(f'datasets={datasets.__version__}')\n",
    "import transformers\n",
    "print(f'transformers={transformers.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4888e0",
   "metadata": {},
   "source": [
    "## 1. Inspection of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b91b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#\n",
    "dataset = load_dataset(\"UrbanSounds/UrbanSoundsSamples\", split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd7ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ESC50 dataset is one of the very few other datasets on Environmental Sound classification\n",
    "#You could try this as an alternative\n",
    "#dataset = load_dataset(\"confit/esc50-demo\", \"fold1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the dataset\n",
    "#dataset = ds\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff807e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect one sample from \n",
    "example = dataset['audio'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612c868",
   "metadata": {},
   "source": [
    "You may notice that the audio column contains several features. Hereâ€™s what they are:\n",
    "\n",
    "- path: the path to the downloaded (and converted) audio file\n",
    "- array: The decoded audio data, represented as a 1-dimensional NumPy array.\n",
    "- sampling_rate. The sampling rate of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb34b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the audio array\n",
    "array = dataset[\"audio\"][0][\"array\"]\n",
    "sampling_rate = example[\"sampling_rate\"]\n",
    "print(array.shape)\n",
    "print(array)\n",
    "print(type(array))\n",
    "print(sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1deef",
   "metadata": {},
   "source": [
    "## 2. Testing one sample of the UrbanSoundsSamples dataset\n",
    "\n",
    "Instruction: select a random number from the dataset and start listening to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random number to select from dataset\n",
    "import random\n",
    "\n",
    "random_number = random.randint(0, len(dataset['audio']))\n",
    "random_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c2b73",
   "metadata": {},
   "source": [
    "### Runnning it with \"Larger CLAP music and speech\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to load a random number out of the dataset\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from transformers import pipeline\n",
    "import IPython\n",
    "\n",
    "example=dataset['audio'][random_number]\n",
    "audio = dataset[\"audio\"][random_number][\"array\"]\n",
    "\n",
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_music_and_speech\")\n",
    "output = audio_classifier(audio, candidate_labels=[\"Motorcycle\", \"Moped\", 'Claxon','Alarm','Loud people','Talking','Gunshot', 'Slamming door','Music', 'Machine'])\n",
    "print(f'Sample number: {random_number}')\n",
    "print(f'{output[0][\"label\"]} {round(output[0][\"score\"],3)}')\n",
    "print(f'{output[1][\"label\"]} {round(output[1][\"score\"],3)}')\n",
    "print(f'{output[2][\"label\"]} {round(output[2][\"score\"],3)}')\n",
    "IPython.display.Audio(example[\"array\"], rate=example['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304d2e8",
   "metadata": {},
   "source": [
    "### Runnning it with \"Larger CLAP general\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4328d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#larger_clap_general\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from transformers import pipeline\n",
    "import IPython\n",
    "\n",
    "example=dataset['audio'][random_number]\n",
    "audio = dataset[\"audio\"][random_number]['array']\n",
    "\n",
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_general\")\n",
    "output = audio_classifier(audio, candidate_labels=[\"Gunshot\", \"Moped\", 'Moped alarm','Claxon','Screaming', 'Motorcycle','Talking', 'Slamming door','Music', 'Machine'])\n",
    "\n",
    "print(f'Sample number: {random_number}')\n",
    "print(f'{output[0][\"label\"]} {round(output[0][\"score\"],3)}')\n",
    "print(f'{output[1][\"label\"]} {round(output[1][\"score\"],3)}')\n",
    "print(f'{output[2][\"label\"]} {round(output[2][\"score\"],3)}')\n",
    "\n",
    "IPython.display.Audio(example['array'], rate=example['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe765f",
   "metadata": {},
   "source": [
    "## 3. Generating results for the whole dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b108e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a neat function to call the model\n",
    "\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from transformers import pipeline\n",
    "import IPython\n",
    "\n",
    "\n",
    "def call_clap(sample_no, model):\n",
    "    global output\n",
    "    audio_sample = dataset[\"audio\"][sample_no]['array']\n",
    "    audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=model)\n",
    "    output = audio_classifier(audio_sample, candidate_labels=[\"Gunshot\", \"Moped\", 'Moped alarm','Claxon','Screaming', 'Motorcycle','Talking', 'Slamming door','Music', 'Machine'])\n",
    "    return output\n",
    "\n",
    "sample_no = 0\n",
    "call_clap(sample_no, \"laion/larger_clap_general\")\n",
    "\n",
    "print(f'Sample number: {sample_no}')\n",
    "print(f'{output[0][\"label\"]} {round(output[0][\"score\"],3)}')\n",
    "print(f'{output[1][\"label\"]} {round(output[1][\"score\"],3)}')\n",
    "print(f'{output[2][\"label\"]} {round(output[2][\"score\"],3)}')\n",
    "IPython.display.Audio(dataset[\"audio\"][sample_no]['array'], rate=example['sampling_rate']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3abe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list =[] \n",
    "for i in range(len(dataset)-1):\n",
    "    sample_no = i\n",
    "    call_clap(sample_no, \"laion/larger_clap_general\")\n",
    "    print(f'Sample {i+1}: {output[0][\"label\"]} - {round(output[0][\"score\"],3)}')\n",
    "    result_list.append(f'{output[0][\"label\"]} - {round(output[0][\"score\"],3)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753082af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "\n",
    "for i in range(len(result_list)):\n",
    "    print(result_list[i])\n",
    "    audio = Audio(data=dataset[\"audio\"][i]['array'], rate=example['sampling_rate'])\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268c381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
